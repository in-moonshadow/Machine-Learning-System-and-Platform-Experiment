import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
import matplotlib.pyplot as plt
from tqdm import tqdm
import warnings
import os
from pathlib import Path
from typing import Tuple, Dict, Optional
import logging
from dataclasses import dataclass, field
import pickle
from sklearn.utils.class_weight import compute_class_weight

warnings.filterwarnings('ignore')

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


@dataclass
class Config:
    """ä¼˜åŒ–çš„é…ç½®ç±»"""
    # æ•°æ®ç›¸å…³
    data_dir: str = 'ml-100k'
    test_size: float = 0.2
    random_state: int = 42
    implicit_threshold: float = 4.0
    use_cache: bool = True
    cache_dir: str = 'cache'

    # æ¨¡å‹ç›¸å…³
    embedding_dim: int = 128
    hidden_dims: list = field(default_factory=lambda: [256, 128, 64, 32])
    dropout_rates: list = field(default_factory=lambda: [0.4, 0.3, 0.2, 0.1])
    use_batch_norm: bool = True

    # è®­ç»ƒç›¸å…³
    num_epochs: int = 100
    batch_size: int = 512
    test_batch_size: int = 1024
    learning_rate: float = 0.001
    weight_decay: float = 1e-5

    # ä¼˜åŒ–ç­–ç•¥
    use_class_weights: bool = True
    use_label_smoothing: bool = False  # ã€ä¿®æ”¹ã€‘ä¸BCEWithLogitsLossä¸å…¼å®¹
    label_smoothing: float = 0.1

    # å­¦ä¹ ç‡è°ƒåº¦
    scheduler_patience: int = 5
    scheduler_factor: float = 0.5

    # DataLoaderç›¸å…³
    num_workers: int = 0
    prefetch_factor: Optional[int] = None  # ã€ä¿®æ”¹ã€‘æ”¹ä¸ºå¯é€‰
    persistent_workers: bool = False

    # æ—©åœç›¸å…³
    patience: int = 15
    min_delta: float = 0.001

    # è¾“å‡ºç›¸å…³
    save_dir: str = 'outputs'

    # æ··åˆç²¾åº¦è®­ç»ƒ
    use_amp: bool = False  # ã€æ–°å¢ã€‘

    def __post_init__(self):
        """ååˆå§‹åŒ–å¤„ç†"""
        Path(self.save_dir).mkdir(parents=True, exist_ok=True)
        Path(self.cache_dir).mkdir(parents=True, exist_ok=True)

        # ã€ä¿®æ”¹ã€‘è‡ªåŠ¨é…ç½® num_workers
        if self.num_workers == 0:
            import platform
            if platform.system() != 'Windows':
                self.num_workers = min(4, os.cpu_count() or 1)
                self.persistent_workers = self.num_workers > 0
                self.prefetch_factor = 2 if self.num_workers > 0 else None
            else:
                self.prefetch_factor = None


class MovieLensDataset(Dataset):
    """æ”¹è¿›çš„æ•°æ®é›†ç±»"""

    def __init__(self, data: pd.DataFrame, num_users: int, num_items: int,
                 config: Config, is_training: bool = True):
        self.num_users = num_users  # ã€æ–°å¢ã€‘ä¿å­˜ä»¥ä¾¿æ•°æ®å¢å¼ºä½¿ç”¨
        self.num_items = num_items  # ã€æ–°å¢ã€‘
        self.users = torch.tensor(data['user_id'].values, dtype=torch.long)
        self.items = torch.tensor(data['item_id'].values, dtype=torch.long)
        self.ratings = data['rating'].values
        self.is_training = is_training
        self.config = config

        # è®¡ç®—æ ‡ç­¾
        labels = (self.ratings >= config.implicit_threshold).astype(np.float32)
        self.labels = torch.tensor(labels, dtype=torch.float32)

        # ã€æ–°å¢ã€‘è®¡ç®—ç±»åˆ«æƒé‡ï¼ˆä¾›Trainerä½¿ç”¨ï¼‰
        self.class_weights = None
        if is_training and config.use_class_weights:
            self.class_weights = self._compute_class_weights(labels)
            logger.info(f"ç±»åˆ«æƒé‡: è´Ÿç±»={self.class_weights[0]:.4f}, æ­£ç±»={self.class_weights[1]:.4f}")

        logger.info(f"{'è®­ç»ƒ' if is_training else 'æµ‹è¯•'}é›†æ ·æœ¬æ•°: {len(self)}, "
                    f"æ­£æ ·æœ¬æ¯”ä¾‹: {labels.mean():.4f}")

    def _compute_class_weights(self, labels: np.ndarray) -> torch.Tensor:
        """è®¡ç®—ç±»åˆ«æƒé‡"""
        class_weights = compute_class_weight(
            'balanced',
            classes=np.array([0.0, 1.0]),
            y=labels
        )
        return torch.tensor(class_weights, dtype=torch.float32)

    def __len__(self) -> int:
        return len(self.users)

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        user = self.users[idx]
        item = self.items[idx]
        label = self.labels[idx]

        # ã€ä¿®æ”¹ã€‘æ”¹è¿›çš„æ•°æ®å¢å¼ºï¼Œç¡®ä¿ä¸è¶Šç•Œ
        if self.is_training and torch.rand(1).item() < 0.05:  # é™ä½æ¦‚ç‡åˆ°5%
            noise = torch.randint(-2, 3, (1,)).item()
            if torch.rand(1).item() < 0.5:
                user = torch.clamp(user + noise, 0, self.num_users - 1).long()
            else:
                item = torch.clamp(item + noise, 0, self.num_items - 1).long()

        return user, item, label


class MovieLensDataManager:
    """æ•°æ®ç®¡ç†å™¨"""

    def __init__(self, config: Config):
        self.config = config
        self.data_dir = Path(config.data_dir)
        self.cache_dir = Path(config.cache_dir)

    def load_movielens_data(self) -> Optional[Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, int, int]]:
        """åŠ è½½MovieLensæ•°æ®é›†"""

        # å°è¯•ä»ç¼“å­˜åŠ è½½
        if self.config.use_cache:
            cached_data = self._load_from_cache()
            if cached_data is not None:
                logger.info("âœ… ä»ç¼“å­˜åŠ è½½æ•°æ®æˆåŠŸï¼")
                return cached_data

        logger.info("æ­£åœ¨åŠ è½½MovieLens 100Kæ•°æ®é›†...")

        # æ£€æŸ¥å¹¶ä¸‹è½½æ•°æ®
        if not self._check_data_exists():
            if not self._download_data():
                return None

        try:
            # è¯»å–è¯„åˆ†æ•°æ®
            ratings = pd.read_csv(
                self.data_dir / 'u.data',
                sep='\t',
                names=['user_id', 'item_id', 'rating', 'timestamp'],
                dtype={'user_id': np.int32, 'item_id': np.int32, 'rating': np.float32}
            )

            # è¯»å–ç”µå½±ä¿¡æ¯
            items = pd.read_csv(
                self.data_dir / 'u.item',
                sep='|',
                encoding='latin-1',
                header=None,
                usecols=[0, 1],
                names=['item_id', 'title']
            )

            # è¯»å–ç”¨æˆ·ä¿¡æ¯
            users = pd.read_csv(
                self.data_dir / 'u.user',
                sep='|',
                names=['user_id', 'age', 'gender', 'occupation', 'zip_code']
            )

            num_users = int(ratings['user_id'].max())
            num_items = int(ratings['item_id'].max())

            self._print_statistics(ratings, num_users, num_items)

            # ä¿å­˜åˆ°ç¼“å­˜
            if self.config.use_cache:
                self._save_to_cache(ratings, items, users, num_users, num_items)

            return ratings, items, users, num_users, num_items

        except Exception as e:
            logger.error(f"âŒ åŠ è½½æ•°æ®é›†å¤±è´¥: {e}")
            return None

    def _load_from_cache(self) -> Optional[Tuple]:
        """ä»ç¼“å­˜åŠ è½½"""
        cache_file = self.cache_dir / 'movielens_cache.pkl'
        if cache_file.exists():
            try:
                with open(cache_file, 'rb') as f:
                    return pickle.load(f)
            except Exception as e:
                logger.warning(f"âš ï¸ ç¼“å­˜åŠ è½½å¤±è´¥: {e}")
        return None

    def _save_to_cache(self, ratings, items, users, num_users, num_items):
        """ä¿å­˜åˆ°ç¼“å­˜"""
        cache_file = self.cache_dir / 'movielens_cache.pkl'
        try:
            with open(cache_file, 'wb') as f:
                pickle.dump((ratings, items, users, num_users, num_items), f)
            logger.info(f"ğŸ’¾ æ•°æ®å·²ç¼“å­˜è‡³ {cache_file}")
        except Exception as e:
            logger.warning(f"âš ï¸ ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")

    def _check_data_exists(self) -> bool:
        """æ£€æŸ¥æ•°æ®æ˜¯å¦å­˜åœ¨"""
        required_files = ['u.data', 'u.item', 'u.user']
        return all((self.data_dir / f).exists() for f in required_files)

    def _download_data(self) -> bool:
        """ä¸‹è½½æ•°æ®é›†"""
        import urllib.request
        import zipfile

        try:
            url = "http://files.grouplens.org/datasets/movielens/ml-100k.zip"
            logger.info(f"â¬‡ï¸ æ­£åœ¨ä» {url} ä¸‹è½½æ•°æ®é›†...")

            zip_path = "ml-100k.zip"
            urllib.request.urlretrieve(url, zip_path)

            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                zip_ref.extractall(".")

            os.remove(zip_path)
            logger.info("âœ… æ•°æ®é›†ä¸‹è½½å®Œæˆï¼")
            return True

        except Exception as e:
            logger.error(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
            return False

    @staticmethod
    def _print_statistics(ratings: pd.DataFrame, num_users: int, num_items: int):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        num_ratings = len(ratings)
        sparsity = 1 - num_ratings / (num_users * num_items)

        logger.info(f"ğŸ“Š æ•°æ®ç»Ÿè®¡:")
        logger.info(f"  - ç”¨æˆ·æ•°é‡: {num_users:,}")
        logger.info(f"  - ç”µå½±æ•°é‡: {num_items:,}")
        logger.info(f"  - è¯„åˆ†æ•°é‡: {num_ratings:,}")
        logger.info(f"  - æ•°æ®ç¨€ç–åº¦: {sparsity:.4f}")

class MatrixFactorizationModel(nn.Module):
    """çŸ©é˜µåˆ†è§£æ¨¡å‹ (MF)"""

    def __init__(self, num_users: int, num_items: int, config: Config, use_sigmoid: bool = True):
        super().__init__()
        self.user_embeddings = nn.Embedding(num_users + 1, config.embedding_dim, padding_idx=0)
        self.item_embeddings = nn.Embedding(num_items + 1, config.embedding_dim, padding_idx=0)
        self.use_sigmoid = use_sigmoid
        self._init_weights()

    def _init_weights(self):
        nn.init.normal_(self.user_embeddings.weight, std=0.01)
        nn.init.normal_(self.item_embeddings.weight, std=0.01)

    def forward(self, user: torch.Tensor, item: torch.Tensor) -> torch.Tensor:
        user_vec = self.user_embeddings(user)
        item_vec = self.item_embeddings(item)

        if user_vec.dim() > 2:
            user_vec = user_vec.squeeze(1)
            item_vec = item_vec.squeeze(1)

        # ç‚¹ç§¯
        output = (user_vec * item_vec).sum(dim=-1)

        if self.use_sigmoid:
            output = torch.sigmoid(output)

        return output

class NeuralCollaborativeFiltering(nn.Module):
    """ç¥ç»ååŒè¿‡æ»¤ (NCF)"""

    def __init__(self, num_users: int, num_items: int, config: Config, use_sigmoid: bool = True):
        super().__init__()
        # GMFéƒ¨åˆ†
        self.gmf_user_embeddings = nn.Embedding(num_users + 1, config.embedding_dim, padding_idx=0)
        self.gmf_item_embeddings = nn.Embedding(num_items + 1, config.embedding_dim, padding_idx=0)

        # MLPéƒ¨åˆ†
        self.mlp_user_embeddings = nn.Embedding(num_users + 1, config.embedding_dim, padding_idx=0)
        self.mlp_item_embeddings = nn.Embedding(num_items + 1, config.embedding_dim, padding_idx=0)

        # MLPå±‚
        mlp_layers = []
        input_dim = config.embedding_dim * 2
        for hidden_dim in [128, 64, 32]:
            mlp_layers.extend([
                nn.Linear(input_dim, hidden_dim),
                nn.ReLU(),
                nn.Dropout(0.2)
            ])
            input_dim = hidden_dim

        self.mlp = nn.Sequential(*mlp_layers)

        # èåˆå±‚
        self.output_layer = nn.Linear(config.embedding_dim + 32, 1)
        self.use_sigmoid = use_sigmoid

        self._init_weights()

    def _init_weights(self):
        for module in self.modules():
            if isinstance(module, nn.Embedding):
                nn.init.normal_(module.weight, std=0.01)
            elif isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)

    def forward(self, user: torch.Tensor, item: torch.Tensor) -> torch.Tensor:
        # GMFéƒ¨åˆ†
        gmf_user = self.gmf_user_embeddings(user)
        gmf_item = self.gmf_item_embeddings(item)

        if gmf_user.dim() > 2:
            gmf_user = gmf_user.squeeze(1)
            gmf_item = gmf_item.squeeze(1)

        gmf_output = gmf_user * gmf_item

        # MLPéƒ¨åˆ†
        mlp_user = self.mlp_user_embeddings(user)
        mlp_item = self.mlp_item_embeddings(item)

        if mlp_user.dim() > 2:
            mlp_user = mlp_user.squeeze(1)
            mlp_item = mlp_item.squeeze(1)

        mlp_input = torch.cat([mlp_user, mlp_item], dim=-1)
        mlp_output = self.mlp(mlp_input)

        # èåˆ
        concat = torch.cat([gmf_output, mlp_output], dim=-1)
        output = self.output_layer(concat).squeeze()

        if self.use_sigmoid:
            output = torch.sigmoid(output)

        return output

class DeepFMModel(nn.Module):
    """DeepFMæ¨¡å‹"""

    def __init__(self, num_users: int, num_items: int, config: Config, use_sigmoid: bool = True):
        super().__init__()
        # å…±äº«Embedding
        self.user_embeddings = nn.Embedding(num_users + 1, config.embedding_dim, padding_idx=0)
        self.item_embeddings = nn.Embedding(num_items + 1, config.embedding_dim, padding_idx=0)

        # FMéƒ¨åˆ† - ä¸€é˜¶
        self.fm_user_bias = nn.Embedding(num_users + 1, 1, padding_idx=0)
        self.fm_item_bias = nn.Embedding(num_items + 1, 1, padding_idx=0)
        self.fm_global_bias = nn.Parameter(torch.zeros(1))

        # Deepéƒ¨åˆ†
        deep_layers = []
        input_dim = config.embedding_dim * 2
        for hidden_dim in [256, 128, 64]:
            deep_layers.extend([
                nn.Linear(input_dim, hidden_dim),
                nn.ReLU(),
                nn.BatchNorm1d(hidden_dim),
                nn.Dropout(0.3)
            ])
            input_dim = hidden_dim

        self.deep = nn.Sequential(*deep_layers)
        self.deep_output = nn.Linear(64, 1)

        self.use_sigmoid = use_sigmoid
        self._init_weights()

    def _init_weights(self):
        for module in self.modules():
            if isinstance(module, nn.Embedding):
                nn.init.normal_(module.weight, std=0.01)
            elif isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)

    def forward(self, user: torch.Tensor, item: torch.Tensor) -> torch.Tensor:
        # Embedding
        user_emb = self.user_embeddings(user)
        item_emb = self.item_embeddings(item)

        if user_emb.dim() > 2:
            user_emb = user_emb.squeeze(1)
            item_emb = item_emb.squeeze(1)

        # FMä¸€é˜¶
        fm_first_order = (
                self.fm_user_bias(user).squeeze() +
                self.fm_item_bias(item).squeeze() +
                self.fm_global_bias
        )

        # FMäºŒé˜¶ï¼ˆäº¤äº’ï¼‰
        fm_second_order = (user_emb * item_emb).sum(dim=-1)

        # Deepéƒ¨åˆ†
        deep_input = torch.cat([user_emb, item_emb], dim=-1)
        deep_output = self.deep(deep_input)
        deep_output = self.deep_output(deep_output).squeeze()

        # èåˆ
        output = fm_first_order + fm_second_order + deep_output

        if self.use_sigmoid:
            output = torch.sigmoid(output)

        return output

class ModelComparison:
    """æ¨¡å‹å¯¹æ¯”å®éªŒç±»"""

    def __init__(self, config: Config):
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.results = {}

    def get_models(self, num_users: int, num_items: int, use_sigmoid: bool = True):
        """è·å–æ‰€æœ‰å¾…å¯¹æ¯”çš„æ¨¡å‹"""
        models = {
            'MF': MatrixFactorizationModel(num_users, num_items, self.config, use_sigmoid),
            'NCF': NeuralCollaborativeFiltering(num_users, num_items, self.config, use_sigmoid),
            'DeepFM': DeepFMModel(num_users, num_items, self.config, use_sigmoid)        }
        return models

    def train_and_evaluate_model(self, model_name: str, model: nn.Module,
                                 train_loader: DataLoader, test_loader: DataLoader,
                                 train_dataset: MovieLensDataset):
        """è®­ç»ƒå¹¶è¯„ä¼°å•ä¸ªæ¨¡å‹"""
        logger.info(f"\n{'=' * 60}")
        logger.info(f"ğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹: {model_name}")
        logger.info(f"{'=' * 60}")

        trainer = Trainer(model, self.config, train_dataset, self.device)
        history = trainer.fit(train_loader, test_loader)

        # ä¿å­˜ç»“æœ
        self.results[model_name] = {
            'history': history,
            'best_f1': trainer.best_f1,
            'final_metrics': {
                'accuracy': history['test_accuracies'][-1],
                'f1': history['test_f1_scores'][-1],
                'precision': history['test_precisions'][-1],
                'recall': history['test_recalls'][-1]
            }
        }

        # ä¿å­˜æ¨¡å‹
        trainer.save_model(f'{model_name}_best_model.pth')

        return history

    def run_comparison(self, train_loader: DataLoader, test_loader: DataLoader,
                       train_dataset: MovieLensDataset, num_users: int, num_items: int):
        """è¿è¡Œæ‰€æœ‰æ¨¡å‹çš„å¯¹æ¯”å®éªŒ"""
        use_sigmoid = not (self.config.use_class_weights and train_dataset.class_weights is not None)
        models = self.get_models(num_users, num_items, use_sigmoid)

        for model_name, model in models.items():
            try:
                self.train_and_evaluate_model(
                    model_name, model, train_loader, test_loader, train_dataset
                )
            except Exception as e:
                logger.error(f"âŒ æ¨¡å‹ {model_name} è®­ç»ƒå¤±è´¥: {e}")
                continue

        # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
        self.generate_comparison_report()
        self.visualize_comparison()

    def generate_comparison_report(self):
        """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ“Š æ¨¡å‹æ€§èƒ½å¯¹æ¯”æŠ¥å‘Š")
        logger.info("=" * 80)

        # åˆ›å»ºå¯¹æ¯”è¡¨æ ¼
        comparison_data = []
        for model_name, result in self.results.items():
            metrics = result['final_metrics']
            comparison_data.append({
                'æ¨¡å‹': model_name,
                'å‡†ç¡®ç‡': f"{metrics['accuracy']:.4f}",
                'F1åˆ†æ•°': f"{metrics['f1']:.4f}",
                'ç²¾ç¡®ç‡': f"{metrics['precision']:.4f}",
                'å¬å›ç‡': f"{metrics['recall']:.4f}",
                'æœ€ä½³F1': f"{result['best_f1']:.4f}"
            })

        df = pd.DataFrame(comparison_data)
        logger.info(f"\n{df.to_string(index=False)}")

        # ä¿å­˜åˆ°CSV
        df.to_csv(Path(self.config.save_dir) / 'model_comparison.csv', index=False)
        logger.info(f"\nğŸ’¾ å¯¹æ¯”ç»“æœå·²ä¿å­˜è‡³ {self.config.save_dir}/model_comparison.csv")

    def visualize_comparison(self):
        """å¯è§†åŒ–å¯¹æ¯”ç»“æœ"""
        fig = plt.figure(figsize=(20, 12))
        gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)

        # 1. è®­ç»ƒæŸå¤±å¯¹æ¯”
        ax1 = fig.add_subplot(gs[0, :])
        for model_name, result in self.results.items():
            epochs = range(1, len(result['history']['train_losses']) + 1)
            ax1.plot(epochs, result['history']['train_losses'],
                     label=model_name, linewidth=2, marker='o', markersize=3)
        ax1.set_title('è®­ç»ƒæŸå¤±å¯¹æ¯”', fontsize=14, fontweight='bold')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.legend()
        ax1.grid(True, alpha=0.3)

        # 2. F1åˆ†æ•°å¯¹æ¯”
        ax2 = fig.add_subplot(gs[1, 0])
        for model_name, result in self.results.items():
            epochs = range(1, len(result['history']['test_f1_scores']) + 1)
            ax2.plot(epochs, result['history']['test_f1_scores'],
                     label=model_name, linewidth=2)
        ax2.set_title('F1åˆ†æ•°å¯¹æ¯”', fontsize=12, fontweight='bold')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('F1 Score')
        ax2.legend()
        ax2.grid(True, alpha=0.3)

        # 3. å‡†ç¡®ç‡å¯¹æ¯”
        ax3 = fig.add_subplot(gs[1, 1])
        for model_name, result in self.results.items():
            epochs = range(1, len(result['history']['test_accuracies']) + 1)
            ax3.plot(epochs, result['history']['test_accuracies'],
                     label=model_name, linewidth=2)
        ax3.set_title('å‡†ç¡®ç‡å¯¹æ¯”', fontsize=12, fontweight='bold')
        ax3.set_xlabel('Epoch')
        ax3.set_ylabel('Accuracy')
        ax3.legend()
        ax3.grid(True, alpha=0.3)

        # 4. ç²¾ç¡®ç‡å¯¹æ¯”
        ax4 = fig.add_subplot(gs[1, 2])
        for model_name, result in self.results.items():
            epochs = range(1, len(result['history']['test_precisions']) + 1)
            ax4.plot(epochs, result['history']['test_precisions'],
                     label=model_name, linewidth=2)
        ax4.set_title('ç²¾ç¡®ç‡å¯¹æ¯”', fontsize=12, fontweight='bold')
        ax4.set_xlabel('Epoch')
        ax4.set_ylabel('Precision')
        ax4.legend()
        ax4.grid(True, alpha=0.3)

        # 5. å¬å›ç‡å¯¹æ¯”
        ax5 = fig.add_subplot(gs[2, 0])
        for model_name, result in self.results.items():
            epochs = range(1, len(result['history']['test_recalls']) + 1)
            ax5.plot(epochs, result['history']['test_recalls'],
                     label=model_name, linewidth=2)
        ax5.set_title('å¬å›ç‡å¯¹æ¯”', fontsize=12, fontweight='bold')
        ax5.set_xlabel('Epoch')
        ax5.set_ylabel('Recall')
        ax5.legend()
        ax5.grid(True, alpha=0.3)

        # 6. æœ€ç»ˆæŒ‡æ ‡æŸ±çŠ¶å›¾
        ax6 = fig.add_subplot(gs[2, 1:])
        metrics_names = ['å‡†ç¡®ç‡', 'F1åˆ†æ•°', 'ç²¾ç¡®ç‡', 'å¬å›ç‡']
        x = np.arange(len(metrics_names))
        width = 0.15

        for i, (model_name, result) in enumerate(self.results.items()):
            metrics = result['final_metrics']
            values = [metrics['accuracy'], metrics['f1'],
                      metrics['precision'], metrics['recall']]
            ax6.bar(x + i * width, values, width, label=model_name)

        ax6.set_title('æœ€ç»ˆæ€§èƒ½æŒ‡æ ‡å¯¹æ¯”', fontsize=12, fontweight='bold')
        ax6.set_ylabel('åˆ†æ•°')
        ax6.set_xticks(x + width * 2)
        ax6.set_xticklabels(metrics_names)
        ax6.legend()
        ax6.grid(True, alpha=0.3, axis='y')

        plt.savefig(Path(self.config.save_dir) / 'model_comparison.png',
                    dpi=300, bbox_inches='tight')
        logger.info(f"ğŸ“Š å¯¹æ¯”å¯è§†åŒ–å›¾å·²ä¿å­˜è‡³ {self.config.save_dir}/model_comparison.png")
        plt.close()

class Trainer:
    """è®­ç»ƒå™¨"""

    def __init__(self, model: nn.Module, config: Config,
                 train_dataset: MovieLensDataset, device: torch.device):  # ã€ä¿®å¤ã€‘æ·»åŠ å‚æ•°
        self.model = model.to(device)
        self.config = config
        self.device = device
        self.train_dataset = train_dataset

        # ã€æ–°å¢ã€‘æ··åˆç²¾åº¦è®­ç»ƒ
        self.scaler = torch.cuda.amp.GradScaler() if config.use_amp else None

        # ã€ä¿®æ”¹ã€‘ç»Ÿä¸€æŸå¤±å‡½æ•°å¤„ç†
        if config.use_class_weights and train_dataset.class_weights is not None:
            pos_weight = train_dataset.class_weights[1] / train_dataset.class_weights[0]
            self.criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(device))
        else:
            self.criterion = nn.BCELoss()

        self.optimizer = optim.AdamW(
            model.parameters(),
            lr=config.learning_rate,
            weight_decay=config.weight_decay,
            betas=(0.9, 0.999)
        )

        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer,
            mode='max',
            factor=config.scheduler_factor,
            patience=config.scheduler_patience,
            verbose=True,
            min_lr=1e-6
        )

        self.early_stopping = EarlyStopping(config.patience, config.min_delta)
        self.best_f1 = 0.0

    def train_epoch(self, train_loader: DataLoader) -> float:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0.0

        pbar = tqdm(train_loader, desc='Training', leave=False)

        for batch_idx, (user, item, label) in enumerate(pbar):
            user = user.to(self.device)
            item = item.to(self.device)
            label = label.to(self.device)

            self.optimizer.zero_grad()

            # ã€æ–°å¢ã€‘æ··åˆç²¾åº¦è®­ç»ƒæ”¯æŒ
            if self.scaler is not None:
                with torch.cuda.amp.autocast():
                    predictions = self.model(user, item)
                    loss = self.criterion(predictions, label)

                self.scaler.scale(loss).backward()
                self.scaler.unscale_(self.optimizer)
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                predictions = self.model(user, item)
                loss = self.criterion(predictions, label)
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                self.optimizer.step()

            total_loss += loss.item()
            pbar.set_postfix({'loss': f'{loss.item():.4f}'})

        return total_loss / len(train_loader)

    @torch.no_grad()
    def evaluate(self, test_loader: DataLoader) -> Dict[str, float]:
        """è¯„ä¼°æ¨¡å‹"""
        self.model.eval()
        all_predictions = []
        all_labels = []
        all_probabilities = []

        for user, item, label in tqdm(test_loader, desc='Evaluating', leave=False):
            user = user.to(self.device)
            item = item.to(self.device)

            predictions = self.model(user, item)

            # ã€ä¿®æ”¹ã€‘ç»Ÿä¸€å¤„ç†æ¦‚ç‡è¾“å‡º
            if isinstance(self.criterion, nn.BCEWithLogitsLoss):
                probabilities = torch.sigmoid(predictions)
            else:
                probabilities = predictions

            probabilities = probabilities.cpu().numpy()
            binary_preds = (probabilities > 0.5).astype(np.float32)

            all_predictions.extend(binary_preds)
            all_probabilities.extend(probabilities)
            all_labels.extend(label.numpy())

        # å¯»æ‰¾æœ€ä¼˜é˜ˆå€¼
        optimal_threshold = self._find_optimal_threshold(all_probabilities, all_labels)
        optimized_preds = (np.array(all_probabilities) > optimal_threshold).astype(np.float32)

        return {
            'accuracy': accuracy_score(all_labels, all_predictions),
            'optimized_accuracy': accuracy_score(all_labels, optimized_preds),
            'f1': f1_score(all_labels, all_predictions, zero_division=0),
            'optimized_f1': f1_score(all_labels, optimized_preds, zero_division=0),
            'precision': precision_score(all_labels, all_predictions, zero_division=0),
            'recall': recall_score(all_labels, all_predictions, zero_division=0),
            'optimal_threshold': optimal_threshold
        }

    @staticmethod
    def _find_optimal_threshold(probabilities, labels, n_thresholds=100):
        """å¯»æ‰¾æœ€ä¼˜é˜ˆå€¼"""
        best_f1 = 0
        best_threshold = 0.5

        for threshold in np.linspace(0.3, 0.7, n_thresholds):
            preds = (np.array(probabilities) > threshold).astype(np.float32)
            f1 = f1_score(labels, preds, zero_division=0)
            if f1 > best_f1:
                best_f1 = f1
                best_threshold = threshold

        return best_threshold

    def fit(self, train_loader: DataLoader, test_loader: DataLoader) -> Dict:
        """è®­ç»ƒæ¨¡å‹"""
        history = {
            'train_losses': [], 'test_accuracies': [], 'test_f1_scores': [],
            'test_precisions': [], 'test_recalls': [], 'learning_rates': [],
            'optimal_thresholds': []
        }

        for epoch in range(self.config.num_epochs):
            # è®­ç»ƒ
            train_loss = self.train_epoch(train_loader)
            current_lr = self.optimizer.param_groups[0]['lr']

            # è¯„ä¼°
            metrics = self.evaluate(test_loader)

            # è®°å½•å†å²
            history['train_losses'].append(train_loss)
            history['test_accuracies'].append(metrics['accuracy'])
            history['test_f1_scores'].append(metrics['f1'])
            history['test_precisions'].append(metrics['precision'])
            history['test_recalls'].append(metrics['recall'])
            history['learning_rates'].append(current_lr)
            history['optimal_thresholds'].append(metrics['optimal_threshold'])

            # å­¦ä¹ ç‡è°ƒåº¦ï¼ˆä½¿ç”¨æ ‡å‡†F1ï¼Œæ›´ç¨³å®šï¼‰
            self.scheduler.step(metrics['f1'])

            logger.info(
                f"Epoch {epoch + 1}/{self.config.num_epochs} | "
                f"Loss: {train_loss:.4f} | Acc: {metrics['accuracy']:.4f} | "
                f"F1: {metrics['f1']:.4f} (Opt: {metrics['optimized_f1']:.4f}) | "
                f"P: {metrics['precision']:.4f} | R: {metrics['recall']:.4f} | "
                f"Thr: {metrics['optimal_threshold']:.3f} | LR: {current_lr:.6f}"
            )

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if metrics['f1'] > self.best_f1:
                self.best_f1 = metrics['f1']
                self.save_model('best_model.pth')
                logger.info("ğŸ”¥ æ–°çš„æœ€ä½³æ¨¡å‹ï¼")

            # ã€ä¿®å¤ã€‘ä½¿ç”¨EarlyStoppingç±»
            if self.early_stopping(metrics['f1']):
                logger.info(f"ğŸ›‘ æ—©åœè§¦å‘äºepoch {epoch + 1}")
                break

            # å­¦ä¹ ç‡è¿‡ä½åœæ­¢
            if current_lr < 1e-6:
                logger.info("ğŸ›‘ å­¦ä¹ ç‡è¿‡ä½ï¼Œåœæ­¢è®­ç»ƒ")
                break

        return history

    def save_model(self, filename: str):
        """ä¿å­˜æ¨¡å‹"""
        save_path = Path(self.config.save_dir) / filename
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'config': self.config,
            'best_f1': self.best_f1
        }, save_path)

class EarlyStopping:
    """æ—©åœæœºåˆ¶"""

    def __init__(self, patience: int = 5, min_delta: float = 0.0001):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_score = None

    def __call__(self, val_score: float) -> bool:
        if self.best_score is None:
            self.best_score = val_score
            return False

        if val_score < self.best_score + self.min_delta:
            self.counter += 1
            return self.counter >= self.patience
        else:
            self.best_score = val_score
            self.counter = 0
            return False

class Visualizer:
    """å¯è§†åŒ–"""

    @staticmethod
    def plot_training_results(history: Dict, save_path: str = 'training_results.png'):
        """ç»˜åˆ¶è®­ç»ƒç»“æœ"""
        fig, axes = plt.subplots(2, 3, figsize=(18, 10))
        epochs = range(1, len(history['train_losses']) + 1)

        # è®­ç»ƒæŸå¤±
        axes[0, 0].plot(epochs, history['train_losses'], 'b-', linewidth=2)
        axes[0, 0].set_title('è®­ç»ƒæŸå¤±', fontsize=12, fontweight='bold')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].grid(True, alpha=0.3)

        # å‡†ç¡®ç‡
        axes[0, 1].plot(epochs, history['test_accuracies'], 'g-', linewidth=2)
        axes[0, 1].set_title('æµ‹è¯•å‡†ç¡®ç‡', fontsize=12, fontweight='bold')
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('Accuracy')
        axes[0, 1].grid(True, alpha=0.3)

        # F1åˆ†æ•°
        axes[0, 2].plot(epochs, history['test_f1_scores'], 'r-', linewidth=2)
        axes[0, 2].set_title('F1åˆ†æ•°', fontsize=12, fontweight='bold')
        axes[0, 2].set_xlabel('Epoch')
        axes[0, 2].set_ylabel('F1 Score')
        axes[0, 2].grid(True, alpha=0.3)

        # ç²¾ç¡®ç‡å’Œå¬å›ç‡
        axes[1, 0].plot(epochs, history['test_precisions'], 'c-', label='ç²¾ç¡®ç‡', linewidth=2)
        axes[1, 0].plot(epochs, history['test_recalls'], 'm-', label='å¬å›ç‡', linewidth=2)
        axes[1, 0].set_title('ç²¾ç¡®ç‡ä¸å¬å›ç‡', fontsize=12, fontweight='bold')
        axes[1, 0].set_xlabel('Epoch')
        axes[1, 0].set_ylabel('Score')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)

        # å­¦ä¹ ç‡
        axes[1, 1].plot(epochs, history['learning_rates'], 'orange', linewidth=2)
        axes[1, 1].set_title('å­¦ä¹ ç‡å˜åŒ–', fontsize=12, fontweight='bold')
        axes[1, 1].set_xlabel('Epoch')
        axes[1, 1].set_ylabel('Learning Rate')
        axes[1, 1].set_yscale('log')
        axes[1, 1].grid(True, alpha=0.3)

        # æœ€ä¼˜é˜ˆå€¼
        axes[1, 2].plot(epochs, history['optimal_thresholds'], 'purple', linewidth=2)
        axes[1, 2].set_title('æœ€ä¼˜åˆ†ç±»é˜ˆå€¼', fontsize=12, fontweight='bold')
        axes[1, 2].set_xlabel('Epoch')
        axes[1, 2].set_ylabel('Threshold')
        axes[1, 2].grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        logger.info(f"ğŸ“Š è®­ç»ƒç»“æœå›¾å·²ä¿å­˜è‡³ {save_path}")
        plt.close()


def main():
    """ä¸»å‡½æ•° - æ¨¡å‹å¯¹æ¯”å®éªŒç‰ˆæœ¬"""
    logger.info("=" * 80)
    logger.info("ğŸš€ æ·±åº¦å­¦ä¹ æ¨èç³»ç»Ÿæ¨¡å‹å¯¹æ¯”å®éªŒ - MovieLens 100K")
    logger.info("=" * 80)

    # 1. åˆå§‹åŒ–é…ç½®
    config = Config()
    config.num_epochs = 50  # å‡å°‘epochä»¥åŠ å¿«å¯¹æ¯”å®éªŒ

    # 2. åŠ è½½æ•°æ®
    data_manager = MovieLensDataManager(config)
    result = data_manager.load_movielens_data()

    if result is None:
        logger.error("âŒ æ•°æ®åŠ è½½å¤±è´¥ï¼Œç¨‹åºç»ˆæ­¢")
        return

    ratings, items, users, num_users, num_items = result

    # 3. åˆ’åˆ†æ•°æ®é›†
    logger.info("ğŸ“Š æ­£åœ¨åˆ’åˆ†æ•°æ®é›†...")
    train_data, test_data = train_test_split(
        ratings,
        test_size=config.test_size,
        random_state=config.random_state,
        stratify=(ratings['rating'] >= config.implicit_threshold).astype(int)
    )

    # 4. åˆ›å»ºæ•°æ®é›†
    train_dataset = MovieLensDataset(train_data, num_users, num_items, config, is_training=True)
    test_dataset = MovieLensDataset(test_data, num_users, num_items, config, is_training=False)

    # 5. åˆ›å»ºæ•°æ®åŠ è½½å™¨
    dataloader_kwargs = {
        'batch_size': config.batch_size,
        'num_workers': config.num_workers,
        'pin_memory': torch.cuda.is_available(),
    }

    if config.num_workers > 0 and config.prefetch_factor is not None:
        dataloader_kwargs['prefetch_factor'] = config.prefetch_factor
        dataloader_kwargs['persistent_workers'] = config.persistent_workers

    train_loader = DataLoader(train_dataset, shuffle=True, **dataloader_kwargs)
    test_loader = DataLoader(
        test_dataset,
        batch_size=config.test_batch_size,
        shuffle=False,
        num_workers=config.num_workers,
        pin_memory=torch.cuda.is_available(),
    )

    # 6. è¿è¡Œæ¨¡å‹å¯¹æ¯”å®éªŒ
    comparison = ModelComparison(config)
    comparison.run_comparison(train_loader, test_loader, train_dataset, num_users, num_items)

    logger.info("\n" + "=" * 80)
    logger.info("âœ… æ‰€æœ‰æ¨¡å‹å¯¹æ¯”å®éªŒå®Œæˆï¼")
    logger.info("=" * 80)


if __name__ == "__main__":
    main()

